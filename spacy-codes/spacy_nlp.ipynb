{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"D:\\Datascience_workspace_2023\\MY_NLP\\spacy-codes\\data\\countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in --> Namibia\n",
      "in --> South Africa\n",
      "Africa --> Cambodia\n",
      "of --> Kuwait\n",
      "as --> Somalia\n",
      "Somalia --> Haiti\n",
      "Haiti --> Mozambique\n",
      "in --> Somalia\n",
      "for --> Rwanda\n",
      "Britain --> Singapore\n",
      "War --> Sierra Leone\n",
      "of --> Afghanistan\n",
      "invaded --> Iraq\n",
      "in --> Sudan\n",
      "of --> Congo\n",
      "earthquake --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"D:\\Datascience_workspace_2023\\MY_NLP\\spacy-codes\\data\\countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"D:\\Datascience_workspace_2023\\MY_NLP\\spacy-codes\\data\\country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the modified drug pattern\n",
    "pattern = [{\"TEXT\": \"O\", \"IS_SPACE\": True, \"OP\": \"?\"}, {\"TEXT\": \"N\", \"IS_SPACE\": True, \"OP\": \"?\"}, {\"TEXT\": \"1\", \"IS_SPACE\": True, \"OP\": \"?\"}, {\"TEXT\": \"5\", \"IS_SPACE\": True, \"OP\": \"?\"}, {\"TEXT\": \"3\", \"IS_SPACE\": True, \"OP\": \"?\"}]\n",
    "\n",
    "# Initialize the Matcher with the current NLP model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the drug pattern to the matcher\n",
    "matcher.add(\"DrugMatcher\", pattern)\n",
    "\n",
    "# Process the text\n",
    "text = \"The patient received O N 15 and ON-15 medications.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Use the matcher on the processed text\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Extract matched spans from the document\n",
    "drug_spans = [doc[start:end] for _, start, end in matches]\n",
    "\n",
    "# Print the matched drug spans\n",
    "for span in drug_spans:\n",
    "    print(span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E178] Each pattern should be a list of dicts, but got: {'LOWER': 'start'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('PatternSequence', [pattern])`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Datascience_workspace_2023\\MY_NLP\\spacy-codes\\spacy_nlp.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pattern_sequence \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mLOWER\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mLOWER\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Add the pattern sequence to the matcher\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m matcher\u001b[39m.\u001b[39;49madd(\u001b[39m\"\u001b[39;49m\u001b[39mPatternSequence\u001b[39;49m\u001b[39m\"\u001b[39;49m, pattern_sequence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Example text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Datascience_workspace_2023/MY_NLP/spacy-codes/spacy_nlp.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe journey starts with a beautiful sunrise and ends with a warm embrace.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Datascience_workspace_2023\\MY_NLP\\myvenv\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:125\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E178] Each pattern should be a list of dicts, but got: {'LOWER': 'start'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('PatternSequence', [pattern])`"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a sequence of patterns\n",
    "pattern_sequence = [\n",
    "    {\"LOWER\": \"start\"},\n",
    "    {\"POS\": \"ADJ\"},\n",
    "    {\"POS\": \"NOUN\"},\n",
    "    {\"LOWER\": \"end\"}\n",
    "]\n",
    "\n",
    "# Add the pattern sequence to the matcher\n",
    "matcher.add(\"PatternSequence\", pattern_sequence)\n",
    "\n",
    "# Example text\n",
    "text = \"The journey starts with a beautiful sunrise and ends with a warm embrace.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply the matcher to find matches\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Extract matched spans and entities\n",
    "matched_sequences = []\n",
    "for match_id, start, end in matches:\n",
    "    label = nlp.vocab.strings[match_id]\n",
    "    sequence_text = doc[start:end].text\n",
    "    matched_sequences.append((sequence_text, label))\n",
    "\n",
    "# Print the identified pattern sequences\n",
    "print(matched_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, I'm Ines! I'm one of the core developers of spaCy, a popular library for advanced Natural Language Processing in Python.\n",
    "\n",
    "In this lesson, we'll take a look at the most important concepts of spaCy and how to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "# contains the processing pipeline\n",
    "# includes language-specific rules for tokenization etc.\n",
    "# The Doc object\n",
    "\n",
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)\n",
    "\n",
    "\n",
    "# Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
    "\n",
    "# To get a token at a specific position, you can index into the doc.\n",
    "\n",
    "# Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_,token.dep_,token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Predicting Named Entities\n",
    "# Process a text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    " \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked as Apple reveals pre-orders\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"IPHONE_X_PATTERN\", [pattern])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
